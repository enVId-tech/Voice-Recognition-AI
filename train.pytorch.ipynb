{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "audio_dir = \"./AUDIO\"\n",
    "csv_file = \"./TEXT/AUDIO.csv\"\n",
    "\n",
    "audio_files = os.listdir(audio_dir)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Audio1.mp3\n",
      "Processed Audio10.mp3\n",
      "Processed Audio11.mp3\n",
      "Processed Audio12.mp3\n",
      "Processed Audio13.mp3\n",
      "Processed Audio14.mp3\n",
      "Processed Audio15.mp3\n",
      "Processed Audio16.mp3\n",
      "Processed Audio17.mp3\n",
      "Processed Audio18.mp3\n",
      "Processed Audio19.mp3\n",
      "Processed Audio2.mp3\n",
      "Processed Audio20.mp3\n",
      "Processed Audio21.mp3\n",
      "Processed Audio22.mp3\n",
      "Processed Audio23.mp3\n",
      "Processed Audio24.mp3\n",
      "Processed Audio25.mp3\n",
      "Processed Audio26.mp3\n",
      "Processed Audio27.mp3\n",
      "Processed Audio28.mp3\n",
      "Processed Audio29.mp3\n",
      "Processed Audio3.mp3\n",
      "Processed Audio30.mp3\n",
      "Processed Audio31.mp3\n",
      "Processed Audio32.mp3\n",
      "Processed Audio33.mp3\n",
      "Processed Audio34.mp3\n",
      "Processed Audio35.mp3\n",
      "Processed Audio36.mp3\n",
      "Processed Audio37.mp3\n",
      "Processed Audio38.mp3\n",
      "Processed Audio39.mp3\n",
      "Processed Audio4.mp3\n",
      "Processed Audio40.mp3\n",
      "Processed Audio41.mp3\n",
      "Processed Audio42.mp3\n",
      "Processed Audio43.mp3\n",
      "Processed Audio44.mp3\n",
      "Processed Audio45.mp3\n",
      "Processed Audio46.mp3\n",
      "Processed Audio47.mp3\n",
      "Processed Audio48.mp3\n",
      "Processed Audio49.mp3\n",
      "Processed Audio5.mp3\n",
      "Processed Audio50.mp3\n",
      "Processed Audio6.mp3\n",
      "Processed Audio7.mp3\n",
      "Processed Audio8.mp3\n",
      "Processed Audio9.mp3\n"
     ]
    }
   ],
   "source": [
    "for file in audio_files:\n",
    "    if not file.endswith(\".mp3\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(audio_dir, file)\n",
    "\n",
    "    y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc = np.transpose(mfcc, (1, 0))\n",
    "\n",
    "    x_train.append(torch.tensor(mfcc))\n",
    "\n",
    "    matched_text = df.loc[df['Video Matching'] == file, 'Text'].values[0]\n",
    "\n",
    "    y_train.append(matched_text)\n",
    "\n",
    "    print(f\"Processed {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  55\n",
      "Characters:  ['u', 'S', 'n', '3', 'W', ':', 'I', 'M', 'y', 'b', '0', 'l', 'f', 'x', 'p', '-', 'o', '\"', 'h', 'H', '1', 'g', 'Y', 'A', 'J', 'q', 'N', 'v', '?', 'O', 'r', ' ', 'B', 'i', 'j', 'R', 'G', 'a', 't', 'e', '_', ',', 'P', 'm', 'E', 'k', 'c', 'd', \"'\", '.', 'T', 'w', '4', 's', '<PAD>']\n",
      "x_train_padded.shape:  (50, 2147, 13)\n",
      "y_train_onehot_padded.shape:  (107350, 2147)\n"
     ]
    }
   ],
   "source": [
    "characters = list(set(char for label in y_train for char in label))\n",
    "characters.append('<PAD>')\n",
    "\n",
    "print(\"Number of characters: \", len(characters))\n",
    "print(\"Characters: \", characters)\n",
    "\n",
    "char_to_id = {char: id for id, char in enumerate(characters)}\n",
    "id_to_char = {id: char for char, id in char_to_id.items()}\n",
    "\n",
    "y_train_ids = [[char_to_id[char] for char in label] for label in y_train]\n",
    "max_len = max(max(len(mfcc) for mfcc in x_train), max(len(label) for label in y_train_ids))\n",
    "\n",
    "y_train_padded_ids = pad_sequences(y_train_ids, maxlen=max_len, padding='post', value=char_to_id['<PAD>'])\n",
    "y_train_padded_ids = y_train_padded_ids.reshape(-1, 1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoder.fit(np.array(list(id_to_char.keys())).reshape(-1, 1))\n",
    "\n",
    "y_train_onehot = onehot_encoder.transform(y_train_padded_ids).toarray()\n",
    "y_train_onehot_padded = pad_sequences(y_train_onehot, maxlen=max_len, padding='post')\n",
    "\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_len, padding='post')\n",
    "\n",
    "print(\"x_train_padded.shape: \", x_train_padded.shape)\n",
    "print(\"y_train_onehot_padded.shape: \", y_train_onehot_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(13, 64, len(characters), num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train_padded).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_onehot_padded).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_train_padded_ids)\n",
    "\n",
    "y_train_onehot = [encoder.transform(label.reshape(-1, 1)).toarray() for label in y_train_padded_ids]\n",
    "y_train_onehot_padded = np.stack(y_train_onehot)\n",
    "new_batch_size = 10000\n",
    "y_train_onehot_padded_subset = y_train_onehot_padded[:new_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 1/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 2/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 3/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 4/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 5/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 6/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 7/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 8/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 9/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 10/100, Loss: 0.09763090312480927\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 11/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 12/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 13/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 14/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 15/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 16/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 17/100, Loss: 0.09763091057538986\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 18/100, Loss: 0.0976310446858406\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 19/100, Loss: 0.09763101488351822\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 20/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 21/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 22/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 23/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 24/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 25/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 26/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 27/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 28/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 29/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 30/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 31/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 32/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 33/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 34/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 35/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 36/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 37/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 38/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 39/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 40/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 41/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 42/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 43/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 44/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 45/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 46/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 47/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 48/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 49/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 50/100, Loss: 0.09763095527887344\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 51/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 52/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 53/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 54/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 55/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 56/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 57/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 58/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 59/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 60/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 61/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 62/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 63/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theli\\Documents\\Visual-Studio-Projects\\Python\\AIProj\\train.pytorch.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(audio_dir, file)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m y, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(file_path, sr\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, mono\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m mfcc \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mmfcc(y\u001b[39m=\u001b[39my, sr\u001b[39m=\u001b[39msr, n_mfcc\u001b[39m=\u001b[39m\u001b[39m13\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m mfcc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(mfcc, (\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m         y, sr_native \u001b[39m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[0;32m    177\u001b[0m     \u001b[39mexcept\u001b[39;00m sf\u001b[39m.\u001b[39mSoundFileRuntimeError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    178\u001b[0m         \u001b[39m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[0;32m    179\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPurePath)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\audio.py:221\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    218\u001b[0m         frame_duration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    220\u001b[0m     \u001b[39m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     y \u001b[39m=\u001b[39m sf_desc\u001b[39m.\u001b[39;49mread(frames\u001b[39m=\u001b[39;49mframe_duration, dtype\u001b[39m=\u001b[39;49mdtype, always_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mT\n\u001b[0;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[39mif\u001b[39;00m frames \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m frames \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(out):\n\u001b[0;32m    894\u001b[0m         frames \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n\u001b[1;32m--> 895\u001b[0m frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_array_io(\u001b[39m'\u001b[39;49m\u001b[39mread\u001b[39;49m\u001b[39m'\u001b[39;49m, out, frames)\n\u001b[0;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m>\u001b[39m frames:\n\u001b[0;32m    897\u001b[0m     \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[39massert\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mitemsize \u001b[39m==\u001b[39m _ffi\u001b[39m.\u001b[39msizeof(ctype)\n\u001b[0;32m   1343\u001b[0m cdata \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mcast(ctype \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m, array\u001b[39m.\u001b[39m__array_interface__[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 1344\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cdata_io(action, cdata, ctype, frames)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1351\u001b[0m     curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtell()\n\u001b[0;32m   1352\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_snd, \u001b[39m'\u001b[39m\u001b[39msf_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m action \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m ctype)\n\u001b[1;32m-> 1353\u001b[0m frames \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file, data, frames)\n\u001b[0;32m   1354\u001b[0m _error_check(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_errorcode)\n\u001b[0;32m   1355\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_amount = 100\n",
    "\n",
    "for epoch in range(epoch_amount):\n",
    "    # Load the model\n",
    "    if os.path.exists(\"model.pth\"):\n",
    "        model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "    outputs = model(x_train_tensor.float())\n",
    "    outputs = outputs.float()\n",
    "\n",
    "    y_train_tensor = y_train_tensor.view(-1).long()\n",
    "\n",
    "    if outputs.shape[0] != y_train_tensor.shape[0]:\n",
    "        y_train_tensor = y_train_tensor[:outputs.shape[0]]\n",
    "\n",
    "    print(\"outputs.shape: \", outputs.shape)\n",
    "\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "    outputs = outputs.argmax(1)\n",
    "\n",
    "    # Get 20 of the audio files to test and check the accuracy\n",
    "    test_files = audio_files[:40]\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for file in test_files:\n",
    "        if not file.endswith(\".mp3\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(audio_dir, file)\n",
    "\n",
    "        y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc = np.transpose(mfcc, (1, 0))\n",
    "\n",
    "        x_test.append(torch.tensor(mfcc))\n",
    "\n",
    "        matched_text = df.loc[df['Video Matching'] == file, 'Text'].values[0]\n",
    "\n",
    "        y_test.append(matched_text)\n",
    "\n",
    "    y_test_ids = [[char_to_id[char] for char in label] for label in y_test]\n",
    "    y_test_padded_ids = pad_sequences(y_test_ids, maxlen=max_len, padding='post', value=char_to_id['<PAD>'])\n",
    "    y_test_padded_ids = y_test_padded_ids.reshape(-1, 1)\n",
    "\n",
    "    y_test_onehot = onehot_encoder.transform(y_test_padded_ids).toarray()\n",
    "    y_test_onehot_padded = pad_sequences(y_test_onehot, maxlen=max_len, padding='post')\n",
    "\n",
    "    x_test_padded = pad_sequences(x_test, maxlen=max_len, padding='post')\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test_padded).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_onehot_padded).to(device)\n",
    "\n",
    "    # Accuracy calculation for the test set\n",
    "    \n",
    "    # Get Audio and Text\n",
    "    outputs = model(x_test_tensor.float())\n",
    "    outputs = outputs.float()\n",
    "\n",
    "    y_test_tensor = y_test_tensor.view(-1).long()\n",
    "\n",
    "    if outputs.shape[0] != y_test_tensor.shape[0]:\n",
    "        y_test_tensor = y_test_tensor[:outputs.shape[0]]\n",
    "\n",
    "    outputs = outputs.argmax(1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == y_test_tensor[i]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    print(\"correct: \", correct)\n",
    "    print(\"total: \", total)\n",
    "\n",
    "    print(\"Accuracy: \", correct / total)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{epoch_amount}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
