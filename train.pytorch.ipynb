{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # This can help diagnose memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vocabulary for character-level tokenization\n",
    "vocab = {'<pad>': 0, '<start>': 1, '<end>': 2}\n",
    "for char in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \":\n",
    "    vocab[char] = len(vocab)\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = len(vocab)  # Vocabulary size\n",
    "batch_size = 2  # Reduced from 16 to 2\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 100\n",
    "max_seq_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechToTextModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SpeechToTextModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
    "        self.fc1 = nn.Linear(64 * (max_seq_length // 16) * (1 * (max_seq_length // 16)), 256)  # Adjusted input size for fc1\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.size())\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSTTDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, max_seq_length):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.audio_dir, self.data.iloc[idx]['Video Matching'])\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "        transcription = self.data.iloc[idx]['Text']\n",
    "\n",
    "        # Tokenize the transcription to integers\n",
    "        tokens = [vocab.get(char, vocab['<pad>']) for char in transcription]\n",
    "\n",
    "        # Pad the sequence to the maximum length\n",
    "        if waveform.size(1) < self.max_seq_length:\n",
    "            padding = torch.zeros((waveform.size(0), self.max_seq_length - waveform.size(1)))\n",
    "            waveform = torch.cat((waveform, padding), dim=1)\n",
    "\n",
    "        return waveform, tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "# Create the model and move it to the GPU (if available)\n",
    "model = SpeechToTextModel(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the dataset with a specified maximum sequence length\n",
    "dataset = CustomSTTDataset(csv_path=\"TEXT/AUDIO.csv\", audio_dir=\"AUDIO\", max_seq_length=max_seq_length)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Find the maximum length of any waveform in this batch\n",
    "    max_length = max(waveform.size(1) for waveform, _ in batch)\n",
    "\n",
    "    # Pad all waveforms and targets to the same length\n",
    "    waveforms = []\n",
    "    targets = []\n",
    "\n",
    "    for waveform, tokens in batch:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "        input_length = waveform.size(1)\n",
    "        target_length = len(tokens)\n",
    "\n",
    "        # Pad inputs to the maximum length (max_length)\n",
    "        if input_length < max_length:\n",
    "            padding = torch.zeros((waveform.size(0), max_length - input_length), device=waveform.device)\n",
    "            waveform = torch.cat((waveform, padding), dim=1)\n",
    "\n",
    "        # Pad targets to the maximum length (max_length)\n",
    "        if target_length < max_length:\n",
    "            tokens += [vocab['<pad>']] * (max_length - target_length)\n",
    "\n",
    "        waveforms.append(waveform)\n",
    "        targets.extend(tokens)  # Use extend to flatten the list of targets\n",
    "\n",
    "    return torch.stack(waveforms), targets\n",
    "\n",
    "# Use DataLoader with the modified dataset and the new collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x40897 and 87616x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theli\\Documents\\Visual-Studio-Projects\\Python\\AIProj\\train.pytorch.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(targets)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Pass the padded sequences through your model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets, input_lengths, target_lengths)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\theli\\Documents\\Visual-Studio-Projects\\Python\\AIProj\\train.pytorch.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theli/Documents/Visual-Studio-Projects/Python/AIProj/train.pytorch.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x40897 and 87616x512)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Calculate input_lengths and target_lengths for the CTCLoss\n",
    "        input_lengths = torch.LongTensor([inputs.size(2)] * inputs.size(0)).to(device)\n",
    "        target_lengths = torch.LongTensor([len(targets)] * inputs.size(0)).to(device)  # Use len(targets)\n",
    "\n",
    "        # Convert targets to tensor\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "\n",
    "        # Pass the padded sequences through your model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"stt_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
