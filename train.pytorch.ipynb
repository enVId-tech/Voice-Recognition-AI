{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "audio_dir = \"./AUDIO\"\n",
    "csv_file = \"./TEXT/AUDIO.csv\"\n",
    "\n",
    "audio_files = os.listdir(audio_dir)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Audio1.mp3\n",
      "Processed Audio10.mp3\n",
      "Processed Audio11.mp3\n",
      "Processed Audio12.mp3\n",
      "Processed Audio13.mp3\n",
      "Processed Audio14.mp3\n",
      "Processed Audio15.mp3\n",
      "Processed Audio16.mp3\n",
      "Processed Audio17.mp3\n",
      "Processed Audio18.mp3\n",
      "Processed Audio19.mp3\n",
      "Processed Audio2.mp3\n",
      "Processed Audio20.mp3\n",
      "Processed Audio21.mp3\n",
      "Processed Audio22.mp3\n",
      "Processed Audio23.mp3\n",
      "Processed Audio24.mp3\n",
      "Processed Audio25.mp3\n",
      "Processed Audio26.mp3\n",
      "Processed Audio27.mp3\n",
      "Processed Audio28.mp3\n",
      "Processed Audio29.mp3\n",
      "Processed Audio3.mp3\n",
      "Processed Audio30.mp3\n",
      "Processed Audio31.mp3\n",
      "Processed Audio32.mp3\n",
      "Processed Audio33.mp3\n",
      "Processed Audio34.mp3\n",
      "Processed Audio35.mp3\n",
      "Processed Audio36.mp3\n",
      "Processed Audio37.mp3\n",
      "Processed Audio38.mp3\n",
      "Processed Audio39.mp3\n",
      "Processed Audio4.mp3\n",
      "Processed Audio40.mp3\n",
      "Processed Audio41.mp3\n",
      "Processed Audio42.mp3\n",
      "Processed Audio43.mp3\n",
      "Processed Audio44.mp3\n",
      "Processed Audio45.mp3\n",
      "Processed Audio46.mp3\n",
      "Processed Audio47.mp3\n",
      "Processed Audio48.mp3\n",
      "Processed Audio49.mp3\n",
      "Processed Audio5.mp3\n",
      "Processed Audio50.mp3\n",
      "Processed Audio6.mp3\n",
      "Processed Audio7.mp3\n",
      "Processed Audio8.mp3\n",
      "Processed Audio9.mp3\n"
     ]
    }
   ],
   "source": [
    "for file in audio_files:\n",
    "    if not file.endswith(\".mp3\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(audio_dir, file)\n",
    "\n",
    "    y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc = np.transpose(mfcc, (1, 0))\n",
    "\n",
    "    x_train.append(torch.tensor(mfcc))\n",
    "\n",
    "    matched_text = df.loc[df['Video Matching'] == file, 'Text'].values[0]\n",
    "\n",
    "    y_train.append(matched_text)\n",
    "\n",
    "    print(f\"Processed {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  55\n",
      "Characters:  ['u', 'S', 'n', '3', 'W', ':', 'I', 'M', 'y', 'b', '0', 'l', 'f', 'x', 'p', '-', 'o', '\"', 'h', 'H', '1', 'g', 'Y', 'A', 'J', 'q', 'N', 'v', '?', 'O', 'r', ' ', 'B', 'i', 'j', 'R', 'G', 'a', 't', 'e', '_', ',', 'P', 'm', 'E', 'k', 'c', 'd', \"'\", '.', 'T', 'w', '4', 's', '<PAD>']\n",
      "x_train_padded.shape:  (50, 2147, 13)\n",
      "y_train_onehot_padded.shape:  (107350, 2147)\n"
     ]
    }
   ],
   "source": [
    "characters = list(set(char for label in y_train for char in label))\n",
    "characters.append('<PAD>')\n",
    "\n",
    "print(\"Number of characters: \", len(characters))\n",
    "print(\"Characters: \", characters)\n",
    "\n",
    "char_to_id = {char: id for id, char in enumerate(characters)}\n",
    "id_to_char = {id: char for char, id in char_to_id.items()}\n",
    "\n",
    "y_train_ids = [[char_to_id[char] for char in label] for label in y_train]\n",
    "max_len = max(max(len(mfcc) for mfcc in x_train), max(len(label) for label in y_train_ids))\n",
    "\n",
    "y_train_padded_ids = pad_sequences(y_train_ids, maxlen=max_len, padding='post', value=char_to_id['<PAD>'])\n",
    "y_train_padded_ids = y_train_padded_ids.reshape(-1, 1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoder.fit(np.array(list(id_to_char.keys())).reshape(-1, 1))\n",
    "\n",
    "y_train_onehot = onehot_encoder.transform(y_train_padded_ids).toarray()\n",
    "y_train_onehot_padded = pad_sequences(y_train_onehot, maxlen=max_len, padding='post')\n",
    "\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_len, padding='post')\n",
    "\n",
    "print(\"x_train_padded.shape: \", x_train_padded.shape)\n",
    "print(\"y_train_onehot_padded.shape: \", y_train_onehot_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(13, 64, len(characters), num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train_padded).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_onehot_padded).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_train_padded_ids)\n",
    "\n",
    "y_train_onehot = [encoder.transform(label.reshape(-1, 1)).toarray() for label in y_train_padded_ids]\n",
    "y_train_onehot_padded = np.stack(y_train_onehot)\n",
    "new_batch_size = 10000\n",
    "y_train_onehot_padded_subset = y_train_onehot_padded[:new_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 1/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 2/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 3/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 4/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 5/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 6/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 7/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 8/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 9/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 10/100, Loss: 0.09763090312480927\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 11/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 12/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 13/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 14/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 15/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 16/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 17/100, Loss: 0.09763091057538986\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 18/100, Loss: 0.0976310446858406\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 19/100, Loss: 0.09763101488351822\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 20/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 21/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 22/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 23/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 24/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 25/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 26/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 27/100, Loss: 0.09763098508119583\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 28/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 29/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 30/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 31/100, Loss: 0.09763102233409882\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 32/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 33/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 34/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 35/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 36/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 37/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 38/100, Loss: 0.09763099998235703\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 39/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 40/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 41/100, Loss: 0.09763100743293762\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 42/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 43/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 44/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 45/100, Loss: 0.09763097763061523\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 46/100, Loss: 0.09763097018003464\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 47/100, Loss: 0.09763094782829285\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 48/100, Loss: 0.09763103723526001\n",
      "outputs.shape:  torch.Size([50, 55])\n",
      "correct:  39\n",
      "total:  40\n",
      "Accuracy:  0.975\n",
      "Epoch: 49/100, Loss: 0.09763094037771225\n",
      "outputs.shape:  torch.Size([50, 55])\n"
     ]
    }
   ],
   "source": [
    "epoch_amount = 100\n",
    "\n",
    "for epoch in range(epoch_amount):\n",
    "    # Load the model\n",
    "    if os.path.exists(\"model.pth\"):\n",
    "        model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "    outputs = model(x_train_tensor.float())\n",
    "    outputs = outputs.float()\n",
    "\n",
    "    y_train_tensor = y_train_tensor.view(-1).long()\n",
    "\n",
    "    if outputs.shape[0] != y_train_tensor.shape[0]:\n",
    "        y_train_tensor = y_train_tensor[:outputs.shape[0]]\n",
    "\n",
    "    print(\"outputs.shape: \", outputs.shape)\n",
    "\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "    outputs = outputs.argmax(1)\n",
    "\n",
    "    # Get 20 of the audio files to test and check the accuracy\n",
    "    test_files = audio_files[:40]\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for file in test_files:\n",
    "        if not file.endswith(\".mp3\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(audio_dir, file)\n",
    "\n",
    "        y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc = np.transpose(mfcc, (1, 0))\n",
    "\n",
    "        x_test.append(torch.tensor(mfcc))\n",
    "\n",
    "        matched_text = df.loc[df['Video Matching'] == file, 'Text'].values[0]\n",
    "\n",
    "        y_test.append(matched_text)\n",
    "\n",
    "    y_test_ids = [[char_to_id[char] for char in label] for label in y_test]\n",
    "    y_test_padded_ids = pad_sequences(y_test_ids, maxlen=max_len, padding='post', value=char_to_id['<PAD>'])\n",
    "    y_test_padded_ids = y_test_padded_ids.reshape(-1, 1)\n",
    "\n",
    "    y_test_onehot = onehot_encoder.transform(y_test_padded_ids).toarray()\n",
    "    y_test_onehot_padded = pad_sequences(y_test_onehot, maxlen=max_len, padding='post')\n",
    "\n",
    "    x_test_padded = pad_sequences(x_test, maxlen=max_len, padding='post')\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test_padded).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_onehot_padded).to(device)\n",
    "\n",
    "    # Accuracy calculation for the test set\n",
    "    \n",
    "    # Get Audio and Text\n",
    "    outputs = model(x_test_tensor.float())\n",
    "    outputs = outputs.float()\n",
    "\n",
    "    y_test_tensor = y_test_tensor.view(-1).long()\n",
    "\n",
    "    if outputs.shape[0] != y_test_tensor.shape[0]:\n",
    "        y_test_tensor = y_test_tensor[:outputs.shape[0]]\n",
    "\n",
    "    outputs = outputs.argmax(1)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == y_test_tensor[i]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    print(\"correct: \", correct)\n",
    "    print(\"total: \", total)\n",
    "\n",
    "    print(\"Accuracy: \", correct / total)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{epoch_amount}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "t