{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "audio_dir = \"./AUDIO\"\n",
    "csv_file = \"./TEXT/AUDIO.csv\"\n",
    "\n",
    "audio_files = os.listdir(audio_dir)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "df = pd.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    if not file.endswith(\".mp3\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(audio_dir, file)\n",
    "\n",
    "    y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc = np.transpose(mfcc, (1, 0))\n",
    "\n",
    "    x_train.append(torch.tensor(mfcc))\n",
    "\n",
    "    matched_text = df.loc[df['Video Matching'] == file, 'Text'].values[0]\n",
    "\n",
    "    y_train.append(matched_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(set(char for label in y_train for char in label))\n",
    "characters.append('<PAD>')\n",
    "\n",
    "char_to_id = {char: id for id, char in enumerate(characters)}\n",
    "id_to_char = {id: char for char, id in char_to_id.items()}\n",
    "\n",
    "y_train_ids = [[char_to_id[char] for char in label] for label in y_train]\n",
    "max_len = max(max(len(mfcc) for mfcc in x_train), max(len(label) for label in y_train_ids))\n",
    "\n",
    "y_train_padded_ids = pad_sequences(y_train_ids, maxlen=max_len, padding='post', value=char_to_id['<PAD>'])\n",
    "y_train_padded_ids = y_train_padded_ids.reshape(-1, 1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoder.fit(np.array(list(id_to_char.keys())).reshape(-1, 1))\n",
    "\n",
    "y_train_onehot = onehot_encoder.transform(y_train_padded_ids).toarray()\n",
    "y_train_onehot_padded = pad_sequences(y_train_onehot, maxlen=max_len, padding='post')\n",
    "\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(13, 64, len(characters), num_layers=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train_padded).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_onehot_padded).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_train_padded_ids)\n",
    "\n",
    "y_train_onehot = [encoder.transform(label.reshape(-1, 1)).toarray() for label in y_train_padded_ids]\n",
    "y_train_onehot_padded = np.stack(y_train_onehot)\n",
    "new_batch_size = 10000\n",
    "y_train_onehot_padded_subset = y_train_onehot_padded[:new_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 2/100, Loss: 0.097631074488163, Accuracy: 0.975\n",
      "Epoch: 3/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 4/100, Loss: 0.0976310446858406, Accuracy: 0.975\n",
      "Epoch: 5/100, Loss: 0.09763094782829285, Accuracy: 0.975\n",
      "Epoch: 6/100, Loss: 0.09763099998235703, Accuracy: 0.975\n",
      "Epoch: 7/100, Loss: 0.09763112664222717, Accuracy: 0.975\n",
      "Epoch: 8/100, Loss: 0.09763111919164658, Accuracy: 0.975\n",
      "Epoch: 9/100, Loss: 0.097631074488163, Accuracy: 0.975\n",
      "Epoch: 10/100, Loss: 0.09763101488351822, Accuracy: 0.975\n",
      "Epoch: 11/100, Loss: 0.097631074488163, Accuracy: 0.975\n",
      "Epoch: 12/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 13/100, Loss: 0.0976310446858406, Accuracy: 0.975\n",
      "Epoch: 14/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 15/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 16/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 17/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 18/100, Loss: 0.0976310670375824, Accuracy: 0.975\n",
      "Epoch: 19/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 20/100, Loss: 0.09763098508119583, Accuracy: 0.975\n",
      "Epoch: 21/100, Loss: 0.097631074488163, Accuracy: 0.975\n",
      "Epoch: 22/100, Loss: 0.09763095527887344, Accuracy: 0.975\n",
      "Epoch: 23/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 24/100, Loss: 0.09763108938932419, Accuracy: 0.975\n",
      "Epoch: 25/100, Loss: 0.09763101488351822, Accuracy: 0.975\n",
      "Epoch: 26/100, Loss: 0.09763098508119583, Accuracy: 0.975\n",
      "Epoch: 27/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 28/100, Loss: 0.09763111174106598, Accuracy: 0.975\n",
      "Epoch: 29/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 30/100, Loss: 0.09763108938932419, Accuracy: 0.975\n",
      "Epoch: 31/100, Loss: 0.09763099998235703, Accuracy: 0.975\n",
      "Epoch: 32/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 33/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 34/100, Loss: 0.09763111919164658, Accuracy: 0.975\n",
      "Epoch: 35/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 36/100, Loss: 0.09763108193874359, Accuracy: 0.975\n",
      "Epoch: 37/100, Loss: 0.09763097018003464, Accuracy: 0.975\n",
      "Epoch: 38/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 39/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 40/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 41/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 42/100, Loss: 0.09763099998235703, Accuracy: 0.975\n",
      "Epoch: 43/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 44/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 45/100, Loss: 0.0976310670375824, Accuracy: 0.975\n",
      "Epoch: 46/100, Loss: 0.0976310446858406, Accuracy: 0.975\n",
      "Epoch: 47/100, Loss: 0.09763101488351822, Accuracy: 0.975\n",
      "Epoch: 48/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 49/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 50/100, Loss: 0.09763101488351822, Accuracy: 0.975\n",
      "Epoch: 51/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 52/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 53/100, Loss: 0.09763111174106598, Accuracy: 0.975\n",
      "Epoch: 54/100, Loss: 0.097631074488163, Accuracy: 0.975\n",
      "Epoch: 55/100, Loss: 0.0976310670375824, Accuracy: 0.975\n",
      "Epoch: 56/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 57/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 58/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 59/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 60/100, Loss: 0.09763098508119583, Accuracy: 0.975\n",
      "Epoch: 61/100, Loss: 0.09763095527887344, Accuracy: 0.975\n",
      "Epoch: 62/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 63/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 64/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 65/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 66/100, Loss: 0.09763095527887344, Accuracy: 0.975\n",
      "Epoch: 67/100, Loss: 0.09763108193874359, Accuracy: 0.975\n",
      "Epoch: 68/100, Loss: 0.09763099998235703, Accuracy: 0.975\n",
      "Epoch: 69/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 70/100, Loss: 0.09763108193874359, Accuracy: 0.975\n",
      "Epoch: 71/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 72/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 73/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 74/100, Loss: 0.0976310446858406, Accuracy: 0.975\n",
      "Epoch: 75/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 76/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 77/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 78/100, Loss: 0.09763095527887344, Accuracy: 0.975\n",
      "Epoch: 79/100, Loss: 0.09763108193874359, Accuracy: 0.975\n",
      "Epoch: 80/100, Loss: 0.09763100743293762, Accuracy: 0.975\n",
      "Epoch: 81/100, Loss: 0.09763108193874359, Accuracy: 0.975\n",
      "Epoch: 82/100, Loss: 0.09763099998235703, Accuracy: 0.975\n",
      "Epoch: 83/100, Loss: 0.09763098508119583, Accuracy: 0.975\n",
      "Epoch: 84/100, Loss: 0.09763101488351822, Accuracy: 0.975\n",
      "Epoch: 85/100, Loss: 0.09763108938932419, Accuracy: 0.975\n",
      "Epoch: 86/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 87/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 88/100, Loss: 0.09763111174106598, Accuracy: 0.975\n",
      "Epoch: 89/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 90/100, Loss: 0.09763097763061523, Accuracy: 0.975\n",
      "Epoch: 91/100, Loss: 0.09763095527887344, Accuracy: 0.975\n",
      "Epoch: 92/100, Loss: 0.09763108193874359, Accuracy: 0.975\n",
      "Epoch: 93/100, Loss: 0.0976310521364212, Accuracy: 0.975\n",
      "Epoch: 94/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 95/100, Loss: 0.09763112664222717, Accuracy: 0.975\n",
      "Epoch: 96/100, Loss: 0.0976310446858406, Accuracy: 0.975\n",
      "Epoch: 97/100, Loss: 0.09763102233409882, Accuracy: 0.975\n",
      "Epoch: 98/100, Loss: 0.09763110429048538, Accuracy: 0.975\n",
      "Epoch: 99/100, Loss: 0.09763103723526001, Accuracy: 0.975\n",
      "Epoch: 100/100, Loss: 0.0976310670375824, Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "epoch_amount = 100\n",
    "\n",
    "for epoch in range(epoch_amount):\n",
    "    outputs = model(x_train_tensor.float())\n",
    "    outputs = outputs.float()\n",
    "\n",
    "    y_train_tensor = y_train_tensor.view(-1).long()\n",
    "\n",
    "    if outputs.shape[0] != y_train_tensor.shape[0]:\n",
    "        y_train_tensor = y_train_tensor[:outputs.shape[0]]\n",
    "\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    outputs = outputs.argmax(1)\n",
    "\n",
    "    # Get 20 of the audio files to test and check the accuracy\n",
    "    test_files = audio_files[:40]\n",
    "\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for file in test_files:\n",
    "        if not file.endswith(\".mp3\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(audio_dir, file)\n",
    "\n",
    "        y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc = np.transpose(mfcc, (1, 0))\n",
    "\n",
    "        x_test.append(torch.tensor(mfcc))\n",
    "\n",
    "        matched_text = df.loc[df['Video Matching'] == file, 'Text'].values[0]\n",
    "\n",
    "        y_test.append(matched_text)\n",
    "\n",
    "    y_test_ids = [[char_to_id[char] for char in label] for label in y_test]\n",
    "    y_test_padded_ids = pad_sequences(y_test_ids, maxlen=max_len, padding='post', value=char_to_id['<PAD>'])\n",
    "    y_test_padded_ids = y_test_padded_ids.reshape(-1, 1)\n",
    "\n",
    "    y_test_onehot = onehot_encoder.transform(y_test_padded_ids).toarray()\n",
    "    y_test_onehot_padded = pad_sequences(y_test_onehot, maxlen=max_len, padding='post')\n",
    "\n",
    "    x_test_padded = pad_sequences(x_test, maxlen=max_len, padding='post')\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test_padded).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_onehot_padded).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test_tensor.float())\n",
    "        outputs = outputs.float()\n",
    "\n",
    "        y_test_tensor = y_test_tensor.view(-1).long()\n",
    "\n",
    "        if outputs.shape[0] != y_test_tensor.shape[0]:\n",
    "            y_test_tensor = y_test_tensor[:outputs.shape[0]]\n",
    "\n",
    "        accuracy = (outputs.argmax(1) == y_test_tensor).sum().item() / y_test_tensor.shape[0]\n",
    "        print(f\"Epoch: {epoch + 1}/{epoch_amount}, Loss: {loss.item()}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
